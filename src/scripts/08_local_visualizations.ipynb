{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY')\n",
    "aws_secretkey_id = os.getenv('AWS_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secretkey_id)\n",
    "bucket = 'cubix-chicago-taxi-zsigy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_s3(bucket: str, path: str, filename:str) -> pd.DataFrame:\n",
    "    \"\"\" Downloads a csv file from an s3 bucket.\n",
    "\n",
    "    Args:\n",
    "        bucket (str):\n",
    "            The bucket where the file is.\n",
    "        path (str):\n",
    "            The folder of the file.\n",
    "        filename (str):\n",
    "            The name of the file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            The DataFrame of the downloaded file.\n",
    "    \"\"\"\n",
    "\n",
    "    full_path = f'{path}{filename}'\n",
    "    object = s3.get_object(Bucket=bucket, Key=full_path)\n",
    "    object = object['Body'].read().decode('utf-8')\n",
    "    # with StringIO the Body of the object behaves as a file\n",
    "    df = pd.read_csv(StringIO(object))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_areas_path = 'transformed_data/community_areas/'\n",
    "company_path = 'transformed_data/company/'\n",
    "date_path = 'transformed_data/date/'\n",
    "payment_type_path = 'transformed_data/payment_types/'\n",
    "taxi_trips_path = 'transformed_data/taxi_trips/'\n",
    "weather_path = 'transformed_data/weather/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   area_code  community_name\n",
      "0          1     Rogers Park\n",
      "1          2      West Ridge\n",
      "2          3          Uptown\n",
      "3          4  Lincoln Square\n",
      "4          5    North Center\n",
      "   company_id                       company\n",
      "0           1                   5 Star Taxi\n",
      "1           2     Taxi Affiliation Services\n",
      "2           3                     Flash Cab\n",
      "3           4                      Sun Taxi\n",
      "4           5  Taxicab Insurance Agency Llc\n",
      "         Date  year  month  day  day_of_week  is_week_end\n",
      "0  2023-01-01  2023      1    1            7         True\n",
      "1  2023-01-02  2023      1    2            1        False\n",
      "2  2023-01-03  2023      1    3            2        False\n",
      "3  2023-01-04  2023      1    4            3        False\n",
      "4  2023-01-05  2023      1    5            4        False\n",
      "   payment_type_id payment_type\n",
      "0                1       Mobile\n",
      "1                2         Cash\n",
      "2                3  Credit Card\n",
      "3                4       Prcard\n",
      "4                5      Unknown\n"
     ]
    }
   ],
   "source": [
    "community_areas = read_csv_from_s3(bucket= bucket, path= community_areas_path, filename= 'community_areas_master.csv')\n",
    "print(community_areas.head())\n",
    "company = read_csv_from_s3(bucket= bucket, path= company_path, filename='company_master.csv')\n",
    "print(company.head())\n",
    "date = read_csv_from_s3(bucket= bucket, path= date_path, filename= 'date_dimension.csv')\n",
    "print(date.head())\n",
    "payment_types = read_csv_from_s3(bucket= bucket, path= payment_type_path, filename= 'payment_type_master.csv')\n",
    "print(payment_types.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_list = []\n",
    "weather_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxi_2023-11-18.csv has been added.\n",
      "taxi_2023-11-19.csv has been added.\n",
      "taxi_2023-11-20.csv has been added.\n",
      "taxi_2023-11-21.csv has been added.\n",
      "taxi_2023-11-22.csv has been added.\n",
      "taxi_2023-11-23.csv has been added.\n",
      "taxi_2023-11-24.csv has been added.\n",
      "taxi_2023-11-25.csv has been added.\n",
      "taxi_2023-11-26.csv has been added.\n",
      "taxi_2023-11-27.csv has been added.\n",
      "taxi_2023-11-28.csv has been added.\n",
      "taxi_2023-11-29.csv has been added.\n",
      "taxi_2023-11-30.csv has been added.\n",
      "taxi_2023-12-01.csv has been added.\n",
      "taxi_2023-12-02.csv has been added.\n",
      "taxi_2023-12-03.csv has been added.\n",
      "taxi_2023-12-04.csv has been added.\n",
      "taxi_2023-12-05.csv has been added.\n",
      "taxi_2023-12-06.csv has been added.\n"
     ]
    }
   ],
   "source": [
    "for file in s3.list_objects(Bucket = bucket, Prefix = taxi_trips_path)['Contents']:\n",
    "    taxi_trip_file_key = file['Key']\n",
    "    if taxi_trip_file_key.split('/')[-1].strip() != '':\n",
    "        if taxi_trip_file_key.split('.')[1] == 'csv':\n",
    "            filename = taxi_trip_file_key.split('/')[-1]\n",
    "            trip = read_csv_from_s3(bucket= bucket, path= taxi_trips_path, filename= filename)\n",
    "            trips_list.append(trip)\n",
    "            print(f'{filename} has been added.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = pd.concat(trips_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278740, 20)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather_2023-11-18.csv has been added.\n",
      "weather_2023-11-19.csv has been added.\n",
      "weather_2023-11-20.csv has been added.\n",
      "weather_2023-11-21.csv has been added.\n",
      "weather_2023-11-22.csv has been added.\n",
      "weather_2023-11-23.csv has been added.\n",
      "weather_2023-11-24.csv has been added.\n",
      "weather_2023-11-25.csv has been added.\n",
      "weather_2023-11-26.csv has been added.\n",
      "weather_2023-11-27.csv has been added.\n",
      "weather_2023-11-28.csv has been added.\n",
      "weather_2023-11-29.csv has been added.\n",
      "weather_2023-11-30.csv has been added.\n",
      "weather_2023-12-01.csv has been added.\n",
      "weather_2023-12-02.csv has been added.\n",
      "weather_2023-12-03.csv has been added.\n",
      "weather_2023-12-04.csv has been added.\n",
      "weather_2023-12-05.csv has been added.\n",
      "weather_2023-12-06.csv has been added.\n"
     ]
    }
   ],
   "source": [
    "for file in s3.list_objects(Bucket = bucket, Prefix = weather_path)['Contents']:\n",
    "    weather_file_key = file['Key']\n",
    "    if weather_file_key.split('/')[-1].strip() != '':\n",
    "        if weather_file_key.split('.')[1] == 'csv':       \n",
    "            filename = weather_file_key.split('/')[-1]\n",
    "            weather = read_csv_from_s3(bucket= bucket, path= weather_path, filename= filename)\n",
    "            weather_list.append(weather)\n",
    "            print(f'{filename} has been added.')            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.concat(weather_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 456 entries, 0 to 455\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   datetime       456 non-null    object \n",
      " 1   temperature    456 non-null    float64\n",
      " 2   wind_speed     456 non-null    float64\n",
      " 3   rain           456 non-null    float64\n",
      " 4   precipitation  456 non-null    float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 17.9+ KB\n"
     ]
    }
   ],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_full = pd.merge(trips, weather, left_on='datetime_for_weather', right_on='datetime', how='inner')\n",
    "trips_full = trips_full.drop(columns=['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_full = pd.merge(trips_full, company, left_on='company_id', right_on='company_id', how='inner')\n",
    "trips_full = trips_full.drop(columns=['company_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'company_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9004\\3462816461.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrips_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrips_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompany\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'company_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'company_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrips_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrips_full\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'company_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\zselyigy\\OneDrive - elte.hu\\otthon\\Cubix data engineer\\programs2\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         )\n\u001b[0;32m    168\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    170\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\zselyigy\\OneDrive - elte.hu\\otthon\\Cubix data engineer\\programs2\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\zselyigy\\OneDrive - elte.hu\\otthon\\Cubix data engineer\\programs2\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1287\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1288\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1289\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m                         \u001b[1;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\zselyigy\\OneDrive - elte.hu\\otthon\\Cubix data engineer\\programs2\\venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1840\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1844\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'company_id'"
     ]
    }
   ],
   "source": [
    "trips_full = pd.merge(trips_full, company, left_on='company_id', right_on='company_id', how='inner')\n",
    "trips_full = trips_full.drop(columns=['company_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_full = pd.merge(trips_full, payment_types, left_on='payment_type_id', right_on='payment_type_id', how='inner')\n",
    "trips_full = trips_full.drop(columns=['payment_type_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_full = pd.merge(trips_full, community_areas, left_on='pickup_community_area_id', right_on='area_code', how='inner')\n",
    "trips_full = trips_full.drop(columns=['pickup_community_area_id', 'area_code'])\n",
    "trips_full.rename(columns={'community_name':'pickup_community_area_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_full = pd.merge(trips_full, community_areas, left_on='dropoff_community_area_id', right_on='area_code', how='inner')\n",
    "trips_full = trips_full.drop(columns=['dropoff_community_area_id', 'area_code'])\n",
    "trips_full.rename(columns={'community_name':'dropoff_community_area_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "date['date'] = pd.to_datetime(date['Date'])\n",
    "date.drop(columns=['Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_full['trip_start_timestamp'] = pd.to_datetime(trips_full['trip_start_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_full['trip_start_date'] = trips_full['trip_start_timestamp'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_full['trip_start_date'] = pd.to_datetime(trips_full['trip_start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 278740 entries, 0 to 278739\n",
      "Data columns (total 25 columns):\n",
      " #   Column                       Non-Null Count   Dtype         \n",
      "---  ------                       --------------   -----         \n",
      " 0   trip_id                      278740 non-null  object        \n",
      " 1   taxi_id                      278740 non-null  object        \n",
      " 2   trip_start_timestamp         278740 non-null  datetime64[ns]\n",
      " 3   trip_end_timestamp           278740 non-null  object        \n",
      " 4   trip_seconds                 278740 non-null  int64         \n",
      " 5   trip_miles                   278740 non-null  float64       \n",
      " 6   fare                         278740 non-null  float64       \n",
      " 7   tips                         278740 non-null  float64       \n",
      " 8   tolls                        278740 non-null  float64       \n",
      " 9   extras                       278740 non-null  float64       \n",
      " 10  trip_total                   278740 non-null  float64       \n",
      " 11  pickup_centroid_latitude     278740 non-null  float64       \n",
      " 12  pickup_centroid_longitude    278740 non-null  float64       \n",
      " 13  dropoff_centroid_latitude    278740 non-null  float64       \n",
      " 14  dropoff_centroid_longitude   278740 non-null  float64       \n",
      " 15  datetime_for_weather         278740 non-null  object        \n",
      " 16  temperature                  278740 non-null  float64       \n",
      " 17  wind_speed                   278740 non-null  float64       \n",
      " 18  rain                         278740 non-null  float64       \n",
      " 19  precipitation                278740 non-null  float64       \n",
      " 20  company                      278740 non-null  object        \n",
      " 21  payment_type                 278740 non-null  object        \n",
      " 22  pickup_community_area_name   278740 non-null  object        \n",
      " 23  dropoff_community_area_name  278740 non-null  object        \n",
      " 24  trip_start_date              278740 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(14), int64(1), object(8)\n",
      "memory usage: 53.2+ MB\n"
     ]
    }
   ],
   "source": [
    "trips_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_full = pd.merge(trips_full, date, left_on='trip_start_date', right_on='date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_full.drop(columns=['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>fare</th>\n",
       "      <th>tips</th>\n",
       "      <th>tolls</th>\n",
       "      <th>extras</th>\n",
       "      <th>trip_total</th>\n",
       "      <th>pickup_centroid_latitude</th>\n",
       "      <th>pickup_centroid_longitude</th>\n",
       "      <th>dropoff_centroid_latitude</th>\n",
       "      <th>dropoff_centroid_longitude</th>\n",
       "      <th>datetime_for_weather</th>\n",
       "      <th>temperature</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>rain</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>company</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>pickup_community_area_name</th>\n",
       "      <th>dropoff_community_area_name</th>\n",
       "      <th>trip_start_date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_week_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad6bac72a079e03c8313cc17ace346a1e5c8bb09</td>\n",
       "      <td>314f48fcaf3556a8f1c5a32b497057bb479a7afded63d2...</td>\n",
       "      <td>2023-11-18 23:00:00</td>\n",
       "      <td>2023-11-19T00:00:00.000</td>\n",
       "      <td>372</td>\n",
       "      <td>1.95</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.79</td>\n",
       "      <td>41.922686</td>\n",
       "      <td>-87.649489</td>\n",
       "      <td>41.944227</td>\n",
       "      <td>-87.655998</td>\n",
       "      <td>2023-11-18 23:00:00</td>\n",
       "      <td>6.3</td>\n",
       "      <td>15.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5 Star Taxi</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Lincoln Park</td>\n",
       "      <td>Lake View</td>\n",
       "      <td>2023-11-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58bcbf8f393a80724715f63a14a059c0ea375b34</td>\n",
       "      <td>c38a211cb84a44a24e959493d5842a48dd150b1e947b30...</td>\n",
       "      <td>2023-11-18 17:00:00</td>\n",
       "      <td>2023-11-18T17:15:00.000</td>\n",
       "      <td>548</td>\n",
       "      <td>1.33</td>\n",
       "      <td>9.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>41.922686</td>\n",
       "      <td>-87.649489</td>\n",
       "      <td>41.944227</td>\n",
       "      <td>-87.655998</td>\n",
       "      <td>2023-11-18 17:00:00</td>\n",
       "      <td>7.5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5 Star Taxi</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Lincoln Park</td>\n",
       "      <td>Lake View</td>\n",
       "      <td>2023-11-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fc5d310b903335f7fc74f23a4afa39bfb0628ec2</td>\n",
       "      <td>fb0ce19e30e712c77c57cfdb6ef729c2d2ad73225d9ec3...</td>\n",
       "      <td>2023-11-18 12:00:00</td>\n",
       "      <td>2023-11-18T12:30:00.000</td>\n",
       "      <td>293</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.67</td>\n",
       "      <td>41.922686</td>\n",
       "      <td>-87.649489</td>\n",
       "      <td>41.944227</td>\n",
       "      <td>-87.655998</td>\n",
       "      <td>2023-11-18 12:00:00</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5 Star Taxi</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Lincoln Park</td>\n",
       "      <td>Lake View</td>\n",
       "      <td>2023-11-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b392cc8e597e0bddaf30d83e01a03fcc65b8b361</td>\n",
       "      <td>137d4f780c3823fdc5f26cec160a8c9f8eb952f769c5bc...</td>\n",
       "      <td>2023-11-18 21:00:00</td>\n",
       "      <td>2023-11-18T22:00:00.000</td>\n",
       "      <td>419</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.67</td>\n",
       "      <td>41.922686</td>\n",
       "      <td>-87.649489</td>\n",
       "      <td>41.944227</td>\n",
       "      <td>-87.655998</td>\n",
       "      <td>2023-11-18 21:00:00</td>\n",
       "      <td>9.6</td>\n",
       "      <td>19.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Flash Cab</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Lincoln Park</td>\n",
       "      <td>Lake View</td>\n",
       "      <td>2023-11-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32eadfa1696cb9444f1f0b50e2e138fa61fff65c</td>\n",
       "      <td>e93472aad9e00c0a523e1a861ae897303548d713ba63d3...</td>\n",
       "      <td>2023-11-18 21:00:00</td>\n",
       "      <td>2023-11-18T21:30:00.000</td>\n",
       "      <td>690</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.52</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>41.922686</td>\n",
       "      <td>-87.649489</td>\n",
       "      <td>41.944227</td>\n",
       "      <td>-87.655998</td>\n",
       "      <td>2023-11-18 21:00:00</td>\n",
       "      <td>9.6</td>\n",
       "      <td>19.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Flash Cab</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Lincoln Park</td>\n",
       "      <td>Lake View</td>\n",
       "      <td>2023-11-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    trip_id  \\\n",
       "0  ad6bac72a079e03c8313cc17ace346a1e5c8bb09   \n",
       "1  58bcbf8f393a80724715f63a14a059c0ea375b34   \n",
       "2  fc5d310b903335f7fc74f23a4afa39bfb0628ec2   \n",
       "3  b392cc8e597e0bddaf30d83e01a03fcc65b8b361   \n",
       "4  32eadfa1696cb9444f1f0b50e2e138fa61fff65c   \n",
       "\n",
       "                                             taxi_id trip_start_timestamp  \\\n",
       "0  314f48fcaf3556a8f1c5a32b497057bb479a7afded63d2...  2023-11-18 23:00:00   \n",
       "1  c38a211cb84a44a24e959493d5842a48dd150b1e947b30...  2023-11-18 17:00:00   \n",
       "2  fb0ce19e30e712c77c57cfdb6ef729c2d2ad73225d9ec3...  2023-11-18 12:00:00   \n",
       "3  137d4f780c3823fdc5f26cec160a8c9f8eb952f769c5bc...  2023-11-18 21:00:00   \n",
       "4  e93472aad9e00c0a523e1a861ae897303548d713ba63d3...  2023-11-18 21:00:00   \n",
       "\n",
       "        trip_end_timestamp  trip_seconds  trip_miles   fare  tips  tolls  \\\n",
       "0  2023-11-19T00:00:00.000           372        1.95  10.00  2.79    0.0   \n",
       "1  2023-11-18T17:15:00.000           548        1.33   9.00  2.00    0.0   \n",
       "2  2023-11-18T12:30:00.000           293        0.00   9.00  1.67    0.0   \n",
       "3  2023-11-18T22:00:00.000           419        0.00   9.00  1.67    0.0   \n",
       "4  2023-11-18T21:30:00.000           690        0.00   9.52  2.41    0.0   \n",
       "\n",
       "   extras  trip_total  pickup_centroid_latitude  pickup_centroid_longitude  \\\n",
       "0     0.0       12.79                 41.922686                 -87.649489   \n",
       "1     0.0       11.00                 41.922686                 -87.649489   \n",
       "2     0.0       10.67                 41.922686                 -87.649489   \n",
       "3     0.0       10.67                 41.922686                 -87.649489   \n",
       "4     0.0       11.93                 41.922686                 -87.649489   \n",
       "\n",
       "   dropoff_centroid_latitude  dropoff_centroid_longitude datetime_for_weather  \\\n",
       "0                  41.944227                  -87.655998  2023-11-18 23:00:00   \n",
       "1                  41.944227                  -87.655998  2023-11-18 17:00:00   \n",
       "2                  41.944227                  -87.655998  2023-11-18 12:00:00   \n",
       "3                  41.944227                  -87.655998  2023-11-18 21:00:00   \n",
       "4                  41.944227                  -87.655998  2023-11-18 21:00:00   \n",
       "\n",
       "   temperature  wind_speed  rain  precipitation      company payment_type  \\\n",
       "0          6.3        15.2   0.0            0.0  5 Star Taxi       Mobile   \n",
       "1          7.5        20.0   0.0            0.0  5 Star Taxi       Mobile   \n",
       "2         -1.3        10.0   0.0            0.0  5 Star Taxi       Mobile   \n",
       "3          9.6        19.4   0.0            0.0    Flash Cab       Mobile   \n",
       "4          9.6        19.4   0.0            0.0    Flash Cab       Mobile   \n",
       "\n",
       "  pickup_community_area_name dropoff_community_area_name trip_start_date  \\\n",
       "0               Lincoln Park                   Lake View      2023-11-18   \n",
       "1               Lincoln Park                   Lake View      2023-11-18   \n",
       "2               Lincoln Park                   Lake View      2023-11-18   \n",
       "3               Lincoln Park                   Lake View      2023-11-18   \n",
       "4               Lincoln Park                   Lake View      2023-11-18   \n",
       "\n",
       "   year  month  day  day_of_week  is_week_end  \n",
       "0  2023     11   18            6         True  \n",
       "1  2023     11   18            6         True  \n",
       "2  2023     11   18            6         True  \n",
       "3  2023     11   18            6         True  \n",
       "4  2023     11   18            6         True  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_full.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
